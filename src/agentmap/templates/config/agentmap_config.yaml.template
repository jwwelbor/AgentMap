# Core configuration
csv_path: "examples/SingleNodeGraph.csv"
autocompile: true
storage_config_path: "agentmap_config_storage.yaml"

# Directory paths
paths:
  custom_agents: "agentmap/custom_agents"
  functions: "agentmap/custom_functions"
  compiled_graphs: "agentmap/compiled_graphs"
  csv_repository: "workflows"  # Directory for storing workflow CSV files

# Memory configuration
memory:
  enabled: false
  default_type: "buffer"  # Options: buffer, buffer_window, summary, token_buffer
  buffer_window_size: 5
  max_token_limit: 2000
  memory_key: "conversation_memory"

# Prompts configuration
prompts:
  directory: "prompts"
  #registry_file: "prompts/registry.yaml"
  enable_cache: false

execution:
  # What to record during execution
  tracking:
    enabled: true
    track_outputs: true
    track_inputs: true
  
  # How to determine success
  success_policy:
    type: "all_nodes"
    critical_nodes: []
    custom_function: ""

# Separate LangChain tracing for debugging
tracing:
  enabled: false
  mode: "local"
  local_exporter: "file"
  local_directory: "./traces"
  project: "your_project_name"  # Replace with your project name`
  langsmith_api_key: ""
  trace_all: true
  trace_graphs: []

# Logging configuration
logging:
  version: 1
  disable_existing_loggers: False
  file_path: null  # Set to a path to enable file logging
  formatters:
    default:
      format: "[%(asctime)s] [%(levelname)s] %(name)s: %(message)s"
  handlers:
    console:
      class: logging.StreamHandler
      formatter: default
      level: DEBUG # TRACE, DEBUG, INFO, WARNING, ERROR
  root:
    level: INFO
    handlers: [console]

# Storage-specific configurations
# storage: look in separate file storage_config.yaml

# LLM configuration
llm:
  openai:
    api_key: ""  # Or use env: OPENAI_API_KEY
    model: "gpt-4o-mini"  # Updated to current model
    temperature: 0.7
  anthropic:
    api_key: ""  # Or use env: ANTHROPIC_API_KEY
    model: "claude-3-5-sonnet-20241022"  # Updated to current model
    temperature: 0.7
  google:
    api_key: ""  # Or use env: GOOGLE_API_KEY
    model: "gemini-1.5-flash"  # Updated to current model
    temperature: 0.7

routing:
  enabled: true
  
  # Provider × Complexity Matrix
  # This is the core routing table that maps (provider, complexity) → model
  routing_matrix:
    anthropic:
      low: "claude-3-5-haiku-20241022"  # Updated to current model
      medium: "claude-3-5-sonnet-20241022"  # Updated to current model
      high: "claude-sonnet-4-20250514"  # Updated to Claude 4
      critical: "claude-opus-4-20250514"  # Updated to Claude 4
    
    openai:
      low: "gpt-4o-mini"  # Updated to current model
      medium: "gpt-4o"  # Updated to current model
      high: "gpt-4o"  # Updated to current model
      critical: "gpt-4o"  # Updated to current model
    
    google:
      low: "gemini-1.5-flash"  # Updated to current model
      medium: "gemini-1.5-pro"  # Updated to current model
      high: "gemini-1.5-pro"  # Updated to current model
      critical: "gemini-1.5-pro"  # Updated to current model
  
  # Application-Specific Task Types
  # These define the business logic for your specific application
  task_types:
    # General purpose (default)
    general:
      description: "General purpose tasks"
      provider_preference: ["anthropic", "openai", "google"]
      default_complexity: "medium"
      complexity_keywords:
        high: ["analyze", "complex", "detailed", "comprehensive"]
        critical: ["urgent", "critical", "important", "decision"]
    
    # Creative content generation
    creative_writing:
      description: "Creative content generation and storytelling"
      provider_preference: ["anthropic", "openai"]  # Anthropic preferred for creativity
      default_complexity: "high"
      complexity_keywords:
        medium: ["write", "create", "story"]
        high: ["creative", "narrative", "imaginative", "artistic"]
        critical: ["masterpiece", "publication", "professional"]
    
    # Technical analysis and coding
    code_analysis:
      description: "Code review, analysis, and technical documentation"
      provider_preference: ["openai", "anthropic"]  # OpenAI preferred for code
      default_complexity: "medium"
      complexity_keywords:
        low: ["simple", "basic", "review"]
        medium: ["analyze", "refactor", "optimize"]
        high: ["architecture", "design", "complex algorithm"]
        critical: ["security", "production", "critical bug"]
    
    # Customer service and dialogue
    customer_service:
      description: "Customer service interactions and support"
      provider_preference: ["anthropic", "openai"]
      default_complexity: "medium"
      complexity_keywords:
        low: ["greeting", "simple question", "faq"]
        medium: ["support", "help", "issue"]
        high: ["complaint", "escalation", "complex problem"]
        critical: ["urgent", "emergency", "executive"]
    
    # Data analysis and insights
    data_analysis:
      description: "Data processing, analysis, and insights"
      provider_preference: ["openai", "google", "anthropic"]
      default_complexity: "medium"
      complexity_keywords:
        low: ["simple", "summary", "basic stats"]
        medium: ["analyze", "trends", "patterns"]
        high: ["deep analysis", "insights", "correlations"]
        critical: ["predictive", "strategic", "business critical"]

  # Complexity Analysis Configuration
  complexity_analysis:
    # Prompt length thresholds (characters)
    prompt_length_thresholds:
      low: 100      # < 100 chars = low complexity
      medium: 300   # 100-300 chars = medium complexity
      high: 800     # 300-800 chars = high complexity
      # > 800 chars = critical complexity
    
    # Enable different analysis methods
    methods:
      prompt_length: true
      keyword_analysis: true
      context_analysis: true
      memory_analysis: true
    
    # Keyword weighting (how much each type of keyword affects complexity)
    keyword_weights:
      complexity_keywords: 0.4
      task_specific_keywords: 0.3
      prompt_structure: 0.3
    
    # Context analysis settings
    context_analysis:
      memory_size_threshold: 10  # Messages in memory that indicate complexity
      input_field_count_threshold: 5  # Number of input fields
      
  # Cost Optimization Settings
  cost_optimization:
    enabled: true
    # Prefer cheaper models when complexity allows
    prefer_cost_effective: true
    # Maximum cost tier (low, medium, high, critical)
    max_cost_tier: "high"
    
  # Fallback Configuration
  fallback:
    # Default provider when preferences unavailable
    default_provider: "anthropic"
    # Default model when routing fails
    default_model: "claude-3-5-haiku-20241022"  # Updated to current model
    # Retry with lower complexity if model unavailable
    retry_with_lower_complexity: true
    
  # Performance Settings
  performance:
    # Cache routing decisions for identical inputs
    enable_routing_cache: true
    # Cache TTL in seconds
    cache_ttl: 300
    # Maximum cache size
    max_cache_size: 1000

# Environment Variable Overrides
# These allow runtime configuration without changing the file
environment_overrides:
  routing_enabled: ${AGENTMAP_ROUTING_ENABLED:true}
  default_task_type: ${AGENTMAP_DEFAULT_TASK_TYPE:general}
  cost_optimization: ${AGENTMAP_COST_OPTIMIZATION:true}
  routing_cache_enabled: ${AGENTMAP_ROUTING_CACHE:true}