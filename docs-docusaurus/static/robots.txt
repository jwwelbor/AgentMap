# Robots.txt for AgentMap Documentation
# Allow all crawlers access to the site

User-agent: *
Allow: /

# Sitemap location
Sitemap: https://jwwelbor.github.io/AgentMap/sitemap.xml

# Optimize crawling
Crawl-delay: 1

# Allow important areas
Allow: /docs/
Allow: /blog/
Allow: /

# Allow search engines to index images
Allow: /img/
Allow: /assets/

# Allow CSS and JS for better rendering
Allow: *.css
Allow: *.js

# Block development files
Disallow: /.docusaurus/
Disallow: /build/
Disallow: /node_modules/
Disallow: /src/

# Block query parameters
Disallow: /*?*

# Block tag pages to reduce duplicate content
Disallow: /tags/

# Specific bot instructions
User-agent: Googlebot
Crawl-delay: 0
Allow: /

User-agent: Bingbot
Crawl-delay: 1
Allow: /

User-agent: DuckDuckBot
Crawl-delay: 1
Allow: /
