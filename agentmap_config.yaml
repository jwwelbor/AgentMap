# Core configuration
csv_path: "examples/SingleNodeGraph.csv"
autocompile: false
storage_config_path: "storage_config.yaml"

# Directory paths
paths:
  custom_agents: "agentmap/agents/custom"
  functions: "agentmap/functions"
  compiled_graphs: "compiled_graphs"

# LLM configuration
llm:
  openai:
    api_key: ""  # Or use env: OPENAI_API_KEY
    model: "gpt-3.5-turbo"
    temperature: 0.7
  anthropic:
    api_key: ""  # Or use env: ANTHROPIC_API_KEY
    model: "claude-3-sonnet-20240229"
    temperature: 0.7
  google:
    api_key: ""  # Or use env: GOOGLE_API_KEY
    model: "gemini-1.0-pro"
    temperature: 0.7

# Memory configuration
memory:
  enabled: false
  default_type: "buffer"  # Options: buffer, buffer_window, summary, token_buffer
  buffer_window_size: 5
  max_token_limit: 2000
  memory_key: "conversation_memory"

# Prompts configuration
prompts:
  directory: "prompts"
  #registry_file: "prompts/registry.yaml"
  enable_cache: false

execution:
  # What to record during execution
  tracking:
    enabled: true
    track_outputs: true
    track_inputs: true
  
  # How to determine success
  success_policy:
    type: "all_nodes"
    critical_nodes: []
    custom_function: ""

# Separate LangChain tracing for debugging
tracing:
  enabled: false
  mode: "local"
  local_exporter: "file"
  local_directory: "./traces"
  project: "your_project_name"  # Replace with your project name`
  langsmith_api_key: ""
  trace_all: true
  trace_graphs: []

# Logging configuration
logging:
  version: 1
  disable_existing_loggers: False
  file_path: null  # Set to a path to enable file logging
  formatters:
    default:
      format: "[%(asctime)s] [%(levelname)s] %(name)s: %(message)s"
  handlers:
    console:
      class: logging.StreamHandler
      formatter: default
      level: DEBUG # TRACE, DEBUG, INFO, WARNING, ERROR
  root:
    level: INFO
    handlers: [console]
# Storage-specific configurations
# storage: look in separate file storage_config.yaml
